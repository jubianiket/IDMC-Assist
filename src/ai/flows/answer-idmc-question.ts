// This is an autogenerated file from Firebase Studio.

'use server';

/**
 * @fileOverview Provides an AI agent to answer questions about Informatica IDMC,
 * allowing model selection and dynamic API key usage.
 *
 * - askIdmcQuestion - A function that allows users to ask questions about Informatica IDMC and receive AI-generated answers.
 * - AskIdmcQuestionInput - The input type for the askIdmcQuestion function.
 * - AskIdmcQuestionOutput - The return type for the askIdmcQuestion function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import type {Plugin} from 'genkit';

const AskIdmcQuestionInputSchema = z.object({
  question: z.string().describe('The question about Informatica IDMC.'),
  modelId: z.string().describe("The identifier for the AI model to use (e.g., 'googleai/gemini-2.0-flash')."),
  apiKey: z.string().optional().describe('Optional API key for the selected model provider.'),
});
export type AskIdmcQuestionInput = z.infer<typeof AskIdmcQuestionInputSchema>;

const AskIdmcQuestionOutputSchema = z.object({
  answer: z.string().describe('The AI-generated answer to the question.'),
});
export type AskIdmcQuestionOutput = z.infer<typeof AskIdmcQuestionOutputSchema>;

export async function askIdmcQuestion(input: AskIdmcQuestionInput): Promise<AskIdmcQuestionOutput> {
  return askIdmcQuestionFlow(input);
}

// Define the core prompt structure once
const basePromptConfig = {
  input: {schema: z.object({ question: z.string() }) },
  output: {schema: AskIdmcQuestionOutputSchema},
  prompt: `You are an AI assistant that helps users learn Informatica IDMC. Answer the following question:\n\nQuestion: {{{question}}}`,
};

const askIdmcQuestionFlow = ai.defineFlow(
  {
    name: 'askIdmcQuestionFlow',
    inputSchema: AskIdmcQuestionInputSchema,
    outputSchema: AskIdmcQuestionOutputSchema,
  },
  async (input) => {
    const { question, modelId, apiKey } = input;

    if (!modelId) {
        throw new Error('modelId is required.');
    }

    if (apiKey) {
      // API key provided, use a temporary, specifically configured Genkit instance for this call
      const { genkit: localGenkit } = await import('genkit');
      let tempPlugins: Plugin<any>[] = [];

      if (modelId.startsWith('googleai/')) {
        const { googleAI } = await import('@genkit-ai/googleai');
        tempPlugins.push(googleAI({ apiKey }));
      } else if (modelId.startsWith('openai/')) {
        // Temporarily disable OpenAI due to installation issues
        throw new Error('OpenAI models are temporarily unavailable due to a configuration issue. Please select a Google AI model.');
        // const { openAI } = await import('@genkit-ai/openai'); // This line causes issues if @genkit-ai/openai is not installed
        // tempPlugins.push(openAI({ apiKey }));
      } else {
        throw new Error(`Unsupported model provider for modelId: ${modelId}. Supported providers: googleai/.`);
      }

      if (tempPlugins.length === 0) {
          throw new Error(`No valid plugin could be configured for modelId: ${modelId}`);
      }

      const tempAi = localGenkit({ plugins: tempPlugins, logLevel: 'warn' });
      
      const tempPrompt = tempAi.definePrompt({
          name: 'askIdmcQuestionTempPrompt', // Unique name for temporary prompt
          ...basePromptConfig
      });

      const { output } = await tempPrompt(
          { question },
          { model: modelId }
      );
      return output!;

    } else {
      // No API key provided, use the global `ai` instance.
      // Assumes global `ai` has plugins configured to pick up keys from .env
      
      // Temporarily restrict to Google AI if OpenAI plugin isn't globally configured due to install issues
      if (modelId.startsWith('openai/')) {
        throw new Error('OpenAI models are temporarily unavailable. Please select a Google AI model or provide an API key for a Google model if the global key is not working.');
      }

      const globalPrompt = ai.definePrompt({
        name: 'askIdmcQuestionGlobalPrompt', 
        ...basePromptConfig
      });

      const { output } = await globalPrompt(
        { question },
        { model: modelId } // Specify the model selected by the user
      );
      return output!;
    }
  }
);
